---
title: "(PSL) Coding Study 1"
output:
  html_document:
    df_print: paged
    toc: yes
  html_notebook:
    theme: readable
    toc: yes
    toc_float: yes
date: "Fall 2021"
---

## Objective
1) Design a KNN algorithm using R 
2) Summarize classification performance using KNN, linear/quadratic regression and Bayes rule.  Training and test data are generated from a bi-variate Gaussian mixture model.   

## Data Generation

This study is related to the simulation study described in Section 2.3.1 (the so-called Scenario 2) of "Elements of Statistical Learning" (ESL).

**Scenario 2**: the two-dimensional data $X \in R^2$ in each class is generated from a mixture of 10 different bivariate Gaussian distributions with uncorrelated components and different means, i.e.,
$$ X \ | \ Y = k, Z = j \quad \sim \mathcal{N}(\mathbf{m}_{kj}, s^2 I_2) 
$$

where $k = 0$ or $1$, and $j=1, 2, \dots, 10$. Set
$$P(Y=k) = 1/2, \quad P(Z=j) = 1/10, \quad s^2 = 1/5.
$$
In other words, given $Y=k$, $X$ follows a mixture distribution with density function

$$\frac{1}{10}  \sum_{j=1}^{10} \left ( \frac{1}{\sqrt{2 \pi s^2}} \right )^2  e^{ -\|\mathbf{x} - \mathbf{m}_{kj} \|^2/(2 s^2)}.
$$


#### Generate Centers

Generate the 10 centers associated with class 1 from one normal distribution and the 10 centers associated with class 0 from another normal distribution as follows:
$$\mathbf{m}_{1j} \sim \mathcal{N} \Big ( \left ( \begin{array}{c} 1 \\ 0 \end{array} \right ), \mathbf{I}_2 \Big ), \quad \mathbf{m}_{0j} \sim \mathcal{N} \Big ( \left ( \begin{array}{c} 0 \\ 1 \end{array} \right ), \mathbf{I}_2 \Big ).
$$
```{r}
set.seed(1093)
```

```{r eval=TRUE}
p = 2;      
csize = 10;     # number of centers
sigma = 1;      # sd for generating the centers 
m1 = matrix(rnorm(csize*p), csize, p)*sigma + 
  cbind( rep(1, csize), rep(0, csize))
m0 = matrix(rnorm(csize*p), csize, p)*sigma + 
  cbind( rep(0, csize), rep(1, csize))
```

#### Generate Data

Following the data generating process described above, write a function `datagen` to return a training sample of size 200 and a test sample of size 10,000 given generated centers. 

```{r}
sim_params = list(
 csize = 10,      # number of centers
 p = 2,           # dimension
 s = sqrt(1/5),   # standard deviation for generating data
 n = 100,         # training size per class
 N = 5000,        # test size per class
 m0 = m0,         # 10 centers for class 0
 m1 = m1         # 10 centers for class 1
)


generate_sim_data = function(sim_params){
  p = sim_params$p
  s = sim_params$s 
  n = sim_params$n 
  N = sim_params$N 
  m1 = sim_params$m1 
  m0 = sim_params$m0
  csize = sim_params$csize
  
  id1 = sample(1:csize, n, replace = TRUE);
  id0 = sample(1:csize, n, replace = TRUE);
  traindata = matrix(rnorm(2*n*p), 2*n, p)*s + rbind(m1[id1,], m0[id0,])
  Ytrain = factor(c(rep(1,n), rep(0,n)))
  shuffle_row_id = sample(1:n)

  id1 = sample(1:csize, N, replace=TRUE);
  id0 = sample(1:csize, N, replace=TRUE); 
  testdata = matrix(rnorm(2*N*p), 2*N, p)*s + rbind(m1[id1,], m0[id0,])
  Ytest = factor(c(rep(1,N), rep(0,N)))
  
  # Return the training/test data along with labels
  list(
  traindata = traindata,
  Ytrain = Ytrain,
  testdata = testdata,
  Ytest = Ytest
  )
}
```

#### Visulize the Data

```{r}
mydata = generate_sim_data(sim_params)
traindata = mydata$train   #note!!! train is really traindata, R allows shortened syntax 
Ytrain = mydata$Ytrain
testdata = mydata$testdata
Ytest = mydata$Ytest

mycol = rep("blue", nrow(traindata))
mycol[Ytrain==0] = "red"
plot(traindata[, 1], traindata[, 2], type = "n", xlab = "", ylab = "")
points(traindata[, 1], traindata[, 2], col = mycol);
points(m1[, 1], m1[, 2], pch = "+", cex = 2, col = "blue");    
points(m0[, 1], m0[, 2], pch = "+", cex = 2, col = "red");   
legend("bottomright", pch = c(1,1), col = c("blue", "red"), 
       legend = c("class 1", "class 0"))  
```


---

## Part I

In the first part of this study, you are asked to:  

1) Write your own KNN code without using any package  
2) Explain how you handle distance ties and voting ties  
3) Test your code with `mydata` when K = 1, 3, 5; compare your results with the ones from the R command `knn`.  

**distance ties** may occur when you have multiple (training) observations that are equidistant from a test observation. 

**voting ties** may occur when K is an even number and you have 50\% of the k-nearest-neighhors from each of the two classes. 

Display your result on `testdata` as the following 2-by-2 table for K = 1, 3, 5 and they should match with the results from `knn`. **Note** that your result will not look the same as the one shown below since the data are different.

```{r}
 
myKNN = function(traindata, testdata, k){
  #initialize prediction vector with -1s
  
  pred_vec = rep(-1, nrow(testdata)) 
  for (i in 1:nrow(testdata)){  #O(nrows_testdata * (nrows_traindata + nrows_traindata*log(nrows_traindata) + K))? -> O(m*nlogn + m*K)
    temp = matrix(testdata[i,], nrow = nrow(traindata), ncol = ncol(traindata), byrow = TRUE) #each row is a duplicate of testdata[i,]
    temp = (traindata - temp)^2
    temp = rowSums(temp)
    temp = sqrt(temp)
    temp = cbind(dist = temp, label = Ytrain) #temp matrix nx2 contains distance and label
    temp = temp[order(temp[,1], decreasing = FALSE),]  #sort is O(nlogn)
    #print(temp)
    label_1 = 0
    label_2 = 0
    majority = NULL
    for (index in 1:k){
      label = temp[index, 2] 
      #label 2 is really label 1 due to R's convention
      if (label == 2){          
        label_2 = label_2 + 1
      }
      #all labels are either 1 or 2
      else{
        label_1 = label_1 + 1
      }
    }
    #case 1: see if there is a tie
    if (label_1 == label_2){
      kth_neighbor = temp[k,2]
      majority = ifelse(kth_neighbor == 2, 1, 2)
    }
    #case2: no tie 
    else{
    majority = ifelse (label_1 > label_2, 1, 2)
    pred_vec[i] = majority
    }
  }
  pred_vec
}
```

```{r}
pred_vec_k1 = myKNN(traindata = traindata, testdata = testdata, k = 1)
pred_vec_k3 = myKNN(traindata = traindata, testdata = testdata, k = 3)
pred_vec_k5 = myKNN(traindata = traindata, testdata = testdata, k = 5)
```

```{r}
library(class)
test.pred = knn(traindata, testdata, Ytrain, k = 1)
table(Ytest, test.pred)
test.pred = knn(traindata, testdata, Ytrain, k = 3)
table(Ytest, test.pred)
test.pred = knn(traindata, testdata, Ytrain, k = 5)
table(Ytest, test.pred)
```

### Part I summary:  

1) *Write your own KNN code without using any packages*; See myKnn() function 
2) *Explain how you handle distance ties and voting ties*; Both distance and voting ties are handled by reducing k by one. The code removes the kth element from the vote so there cannot be a tie.  For two label classification there will always be a winner even with distance ties.  However for multi-label classification, this will not work to break ties.   
3) *Test your code with `mydata` when K = 1, 3, 5; compare your results with the ones from the R command* `knn`.  The results below from myKnn() match the results from R's  knn() algorithm.

```{r}
table(Ytest, pred_vec_k1)
table(Ytest, pred_vec_k3)
table(Ytest, pred_vec_k5)
```

---

## Part II

In the second part of this study, you are asked to use the **same** set of centers `m1` and `m0`, and repeat the data generating process using `generate_sim_data` 50 times. For each dataset, calculate the training and test errors (the averaged 0/1 error) for each the following four procedures:

1) Linear regression with cut-off value 0.5,
2) quadratic regression with cut-off value 0.5,
3) KNN classification with K chosen by 10-fold cross-validation, and 
4) the Bayes rule (assume your know the values of 20 centers and s).
   
> Summarize your results on training errors and test errors graphically, e.g., using boxplot or stripchart. Also report the mean and standard error for the chosen K values.


**Note**: 

* For KNN, you can use the function `knn` from R package `class`, instead of your own KNN function from Part I.

* "report the mean and standard error for the chosen K values" -- For each data set, you have a K value chosen via 10-fold CV. Report the mean and standard error of the 50 K values. 

### Regression
Fit a linear/quadratic regression model on the training data and use cut-off value $.5$ to transform numerical outcomes to binary outcomes. For illustration purpose, let's try quadratic regression on one pair of training/test sets.

```{r}
fit_lin_model = function(sim_data, verbose = FALSE){
  # change Y from factor to numeric
  sim_data$Ytrain = as.numeric(sim_data$Ytrain) - 1
  sim_data$Ytest = as.numeric(sim_data$Ytest) - 1
  
  # fit a quadratic regression model
  model = lm(
    sim_data$Ytrain ~ .,
    as.data.frame(sim_data$traindata)
  )
  if (verbose) {
    print(summary(model))
  }
  
  decision_thresh = 0.5
  train_pred = as.numeric(model$fitted.values > decision_thresh)
 
  test_yhat = predict(
    model,
    newdata=as.data.frame(sim_data$testdata)
  )
  test_pred = as.numeric(test_yhat > decision_thresh)
  
  # return the mean classification errors on training/test sets
  list(
    train_error = sum(sim_data$Ytrain  != train_pred) / length(sim_data$Ytrain),
    test_error = sum(sim_data$Ytest  != test_pred) / 
      length(sim_data$Ytest)
  )  
}
```

```{r}
lin_model = fit_lin_model(mydata, verbose = TRUE)

```


```{r}
fit_qr_model = function(sim_data, verbose=FALSE) {
  
  # change Y from factor to numeric
  sim_data$Ytrain = as.numeric(sim_data$Ytrain) - 1
  sim_data$Ytest = as.numeric(sim_data$Ytest) - 1
  
  # fit a quadratic regression model
  model = lm(
    sim_data$Ytrain ~ 
      V1 + V2 + I(V1^2) + I(V2^2) + V1:V2,
    as.data.frame(sim_data$traindata)
  )
  if (verbose) {
    print(summary(model))
  }
  
  decision_thresh = 0.5
  train_pred = as.numeric(model$fitted.values > decision_thresh)
 
  test_yhat = predict(
    model,
    newdata=as.data.frame(sim_data$testdata)
  )
  test_pred = as.numeric(test_yhat > decision_thresh)
  
  # return the mean classification errors on training/test sets
  list(
    train_error = sum(sim_data$Ytrain  != train_pred) / length(sim_data$Ytrain),
    test_error = sum(sim_data$Ytest  != test_pred) / 
      length(sim_data$Ytest)
  )
}
```

```{r}
qr_output = fit_qr_model(mydata, TRUE)

```

The code above requires your data represented as a `dataframe`. You can also fit a quadratic regression model and perform prediction when your data is represented as a numerical matrix. The code below should give you the same error rates. 

```{r}
fit_qr_model_matrix = function(sim_data) {
  
  # change Y from factor to numeric
  sim_data$Ytrain = as.numeric(sim_data$Ytrain) - 1
  sim_data$Ytest = as.numeric(sim_data$Ytest) - 1
  
  train_matrix = cbind(sim_data$traindata, sim_data$traindata^2, sim_data$traindata[,1] * sim_data$traindata[,2])
  test_matrix = cbind(sim_data$testdata, sim_data$testdata^2, sim_data$testdata[,1] * sim_data$testdata[,2])
  
  # obtain quadratic regression coefs
  coefs = lm(sim_data$Ytrain ~ train_matrix)$coef
  train_yhat = coefs[1] + train_matrix %*% coefs[-1]
  test_yhat = coefs[1] + test_matrix %*% coefs[-1]
  
  decision_thresh = 0.5
  
  train_pred = as.numeric(train_yhat > decision_thresh)
  test_pred = as.numeric(test_yhat > decision_thresh)
  
  # return the mean classification errors on training/test sets
  list(
    train_error = sum(sim_data$Ytrain  != train_pred) / length(sim_data$Ytrain),
    test_error = sum(sim_data$Ytest  != test_pred) / 
      length(sim_data$Ytest)
  )
}
```


### CV-KNN

How to compute the 10-fold CV error with a particular K value? First, randomly divide the training data equally into ten folds, then compute the prediction error on each fold using the KNN classifier trained based on the other nine folds.

Specially, in the code below, we set K = 3 and loop over runId = 1:10 to compute the CV error. For example, when runId = 3, we find the indices of samples in the 3rd fold (stored in testSetIndex), then train a KNN model without data in testSetIndex and finally form prediction on data in testSetIndex.

**Note**: CV errors are computed only on the training data.

```{r}

cvKNNAveErrorRate = function(k, traindata, Ytrain, foldNum){
  #set.seed(1093)
  n = nrow(traindata)
  foldSize = floor(n/foldNum)  
  #create vector of random numbers, 1:n
  myIndex = sample(1 : n) 
  error = 0
  for(runId in 1:foldNum){
    testSetIndex = ((runId-1)*foldSize + 1):(ifelse(runId == foldNum, n, runId*foldSize))
    #randomize testSetIndex with myIndex (vector containing randomized numbers )
    testSetIndex = myIndex[testSetIndex]
    trainX = traindata[-testSetIndex, ]
    trainY = Ytrain[-testSetIndex]
    testX = traindata[testSetIndex, ]
    testY = Ytrain[testSetIndex]
      
    predictY = knn(trainX, testX, trainY, k)
    error = error + sum(predictY != testY)
        
  }  
  error_Ki = error / n
  return (error_Ki)
}
```


```{r}
#provided code:
# foldNum = 10
# n = nrow(traindata)
# foldSize = floor(n/foldNum)  
# K = 3
# error = 0
# myIndex = sample(1 : n)
# for(runId in 1:foldNum){
#   testSetIndex = ((runId-1)*foldSize + 1):(ifelse(runId == foldNum, n, runId*foldSize))
#   testSetIndex = myIndex[testSetIndex]
#   trainX = traindata[-testSetIndex, ]
#   trainY = Ytrain[-testSetIndex]
#   testX = traindata[testSetIndex, ]
#   testY = Ytrain[testSetIndex]
#   predictY = knn(trainX, testX, trainY, K)
#   error = error + sum(predictY != testY) 
# }
# error = error / n
# error
```

In the code above, the 200 training samples are sequentially divided into 10 folds. This could be **problematic** if the order of the training data is not random, e.g., all samples with Y=1 are arranged at the beginning. To avoid this problem, one can read `testSetIndex` from a shuttled index set (1 to n):

```{r}
# provided code:
# myIndex = sample(1 : n)
# for(runId in 1:foldNum){
#   testSetIndex = ((runId-1)*foldSize + 1):(ifelse(runId == foldNum, n, runId*foldSize))
#   testSetIndex = myIndex[testSetIndex]
  
```

You can put the code above in a function, say `cvKNNAveErrorRate`. Then wrote a function `cvKNN` that returns the best K value based on 10-fold CV errors. Specifically, in the code below, we store all possible K values in vector `KVector`, then compute the corresponding CV errors and store them in  `cvErrorRates` (which, for example, can be computed using a for-loop, but `sapply` is used here).

**Note**: it is possible that there are multiple K values that give the smallest CV error; when this happens, the code below picks the largest one among them, since the larger the K value, the simplier the model.

```{r}
cvKNN = function(traindata, Ytrain, foldNum) {
  n = nrow(traindata)
  foldSize = floor(n/foldNum)  
  #KVector = seq(1, (nrow(dataSet) - foldSize), 1)
  KVector = seq(1, (nrow(traindata) - foldSize), 1)
  cvErrorRates = sapply(KVector, cvKNNAveErrorRate, traindata, Ytrain, foldNum)
  #print(cvErrorRates)
  result = list()
  result$bestK = max(KVector[cvErrorRates == min(cvErrorRates)])
  result$cvError = cvErrorRates[KVector == result$bestK]
  return (result)
}
```


### Bayes Rule
When calculating the misclassification rates using the Bayes Rule, we need to repeatedly evaluate the following ratio:

$$
\text{mixnorm_ratio} = \frac{\sum_{j=1}^{10} \exp \{ - \frac{1}{2 s^2} \| \mathbf{m}_{1j} - \mathbf{x} \|^2 \}}{\sum_{j=1}^{10} \exp \{ - \frac{1}{2 s^2} \| \mathbf{m}_{0j} - \mathbf{x} \|^2 \}}
$$
The function below can be used to compute the ratio. 

```{r}
mixnorm = function(x, centers0, centers1, s){
  ## return the density ratio for a point x, where each 
  ## density is a mixture of normal with multiple components
  
  ## x gets passed in as a vector w length = 2
  d1 = sum(exp(-apply((t(centers1) - x)^2, 2, sum) / (2 * s^2)))
  d0 = sum(exp(-apply((t(centers0) - x)^2, 2, sum) / (2 * s^2)))
  
  return (d1 / d0)
}
```

```{r}
bayes_err = function(data, centers0, centers1, s){
  testdata = data$testdata
  traindata = data$traindata
  Ytrain = data$Ytrain
  Ytest = data$Ytest
  #print(testdata)
  Ytest_pred_Bayes = apply(testdata, 1, mixnorm, centers0, centers1, s)
  Ytrain_pred_Bayes = apply(traindata, 1, mixnorm, centers0, centers1, s)
  #print(Ytest_pred_Bayes)
  Ytest_pred_Bayes = as.numeric(Ytest_pred_Bayes > 1)  #why is >1 the criteria? look at numerator of mix ratio
  Ytrain_pred_Bayes = as.numeric(Ytrain_pred_Bayes > 1)
  #print(Ytest_pred_Bayes)
  #print(Ytest)
  #print(Ytest != Ytest_pred_Bayes)
  #table(Ytest, Ytest_pred_Bayes); 
  test.err.Bayes = sum(Ytest !=  Ytest_pred_Bayes) / (length(Ytest_pred_Bayes))
  train.err.Bayes = sum(Ytrain !=  Ytrain_pred_Bayes) / (length(Ytrain_pred_Bayes))
  results = list(bayes_train_err = train.err.Bayes, bayes_test_err = test.err.Bayes)
  return (results)  #note "results" have to be in parentheses 
}

```


```{r, part2_results, cache=TRUE}

#put all error results in a matrix (50 x 8) see pooled_res below
#put all k values in a vector length = 50
matrix_res = matrix(0, ncol = 8, nrow = 50)
kvalues = rep(0,50)

for (i in 1:50){
  mydata = generate_sim_data(sim_params)
  lm_results = fit_lin_model(mydata, verbose = FALSE) #returns both train and test errors
  qr_results = fit_qr_model(mydata, verbose = FALSE)  #returns both train and test errors
  
  traindata = mydata$traindata
  Ytrain = mydata$Ytrain
  Ytest = mydata$Ytest
  
  k = cvKNN(traindata, Ytrain, foldNum = 10)$bestK
  knn_test_pred = knn(traindata, testdata, Ytrain, k)  #this returns test prediction need to calc err
  knn_test_err = sum(knn_test_pred != Ytest)/length(Ytest)
  knn_train_pred = knn(traindata, traindata, Ytrain, k)  #this returns train prediction need to calc err
  knn_train_err = sum(knn_train_pred != Ytrain)/length(Ytrain)
  
  bayes_error = bayes_err(mydata, centers0 = m0, centers1 = m1, s = sim_params$s)

  pooled_res = c(lm_results[[1]], lm_results[[2]],
                 qr_results[[1]], qr_results[[2]],
                 knn_train_err, knn_test_err,
                 bayes_error[[1]], bayes_error[[2]]) 
  kvalues[i] = k
  matrix_res[i, ] = pooled_res
}


```

```{r}
#means of the error and k results
colMeans(matrix_res)
mean(kvalues)
```
```{r}
var_names = c("lm-train", "lm-test", "qr-train", "qr-test", "knn-train", "knn-test", "bayes-train", "bayes-test")
df_models = as.data.frame(matrix_res)
names(df_models) = var_names
head(df_models)


```

```{r boxplot, fig.width = 8, fig.height = 5}
boxplot(df_models, 
        main = 'Box plots of Train/Test Errors by Model Type',
        xlab = 'Model Type', 
        ylab = 'Error',
        col = c('steelblue', 'purple'),
        cex.axis = 0.8,
        pch = 10)
        

```


### Part II summary:  

The results from this simulation show that training error is slightly lower or nearly the same as the test error.  In general, we would expect the training error to be lower because the model has been conditioned on the training data.  This does not apply to the Bayes model since there is no training.  In both the linear and quadratic models, the mean training error was slightly lower than the test error.  In the linear model, the mean training and test errors were `r colMeans(matrix_res)[1]` and `r colMeans(matrix_res)[2]`, respectively, 1% difference. In the quadratic model, the mean training and test errors were `r colMeans(matrix_res)[3]` and `r colMeans(matrix_res)[4]`, respectively, 4% difference.  The KNN model had a much larger percent difference, 29%, bewteen the mean training and test errors, `r colMeans(matrix_res)[5]` and `r colMeans(matrix_res)[6]`, respectively. The Bayes' mean training error, `r colMeans(matrix_res)[7]`, and test error, `r colMeans(matrix_res)[8]`, show a small difference, 3%, as would be expected since there is no training.  As a consequence the training data doesn't have the advantage of prior knowledge compared to the test data.  Also the variance in all models are smaller in the test data versus the training data.  This is attributed to the larger sample size in the test set (n=10000) versus training set (n=200).

According to the plot above, the Bayes model had the lowest test error rate, followed by KNN, quadratic and lastly the linear model. The underlying data is not linear so we would not expect the linear models to do as well.  KNN and Bayes models do not make this assumption about the underlying data.  The quadratic model is able to achieve a lower test error than the linear model due to its the greater flexibility, 6 dof (quadratic model) versus 3 dof (linear model).  

```{r}
mean_k = mean(kvalues)
std_err_k = sd(kvalues)/sqrt(length(kvalues))
mean_k
std_err_k
```
The mean k value is `r mean_k` and its std error is `r std_err_k`.  

---


## Appendix
```{r}
hist(kvalues)
```
